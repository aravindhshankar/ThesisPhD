\newpage
\thispagestyle{empty}

\chapter*{Summary}
\addcontentsline{toc}{part}{Summary}
\setheader{Summary}
\label{Summary}

Advancements in computing hardware are usually followed by emerging applications trying to utilize the new hardware to the maximum. These advances also find their way into science, where they help us push the boundaries of what has been possible so far. 

At the beginning of the thesis, in Chapter \ref{ch:therm}, we contrast the current consensus answer about thermalization in closed quantum systems, the eigenstate thermalization hypothesis (ETH), with a recently discovered operator thermalization hypothesis (OTH) by studying thermalization dynamics in closed unitary quantum systems. We showed how the two are different and yet similar in some regards. No-go conditions imposed by the operator thermalization hypothesis are a feature of the integrability of the theory, and just a slight move away from it would make matrix elements approach their ETH form. Solving these big eigen problems required the firepower of the local computing grid at Leiden University.

One of the most ubiquitous numerical methods in science is Monte Carlo simulation, initially developed by Stanis≈Çaw Ulam while working on nuclear weapons at Los Alamos National Laboratory. The main idea of Monte Carlo is to randomly sample the values of an integrand to compute the approximate value of an integral, revolutionized science and modern-day computing. In Chapter \ref{ch:higgs}, we turn to the lattice gauge theory. Building on the foundations set by Wegner in his realization of the pure $Z_2$ gauge theory and expanding the work of Fradkin and Shenker, we construct $Z_2$ gauge theory coupled to the several matter fields. Even though they are non-interacting, these matter fields lead to exciting phenomena when gauged through the same gauge field. Namely, a new ``registry'' order in the Higgs phase has emerged, meaning that locally different copies of matter fields align their vectors in a parallel and anti-parallel fashion, even in the case of continuous $O(2)$ symmetry of matter fields. Running Monte Carlo simulations for bigger 3D lattice sizes, we have discovered some exciting characteristics of phase transitions previously obscured by the finite-size effects.      

In recent years we have witnessed an overwhelming development of machine-learning techniques inspired by the new generations of graphic cards. The industry predominantly led the research of new applications; nevertheless, these new and exciting applications have found their way into science, especially physics. One method, named neural quantum states (NQS), considers using a neural network to represent a quantum state of highly entangled systems. A particular architecture of neural networks called Restricted Boltzmann machines (RBM) is very well situated for the task, partially because it includes non-local correlation by design. In Chapter \ref{ch:EE}, we explore the entanglement entropy and its scaling of the same gauge theories from Chapter \ref{ch:higgs}, now expressed as a 2D quantum theory in one lower dimension. We find that the expected linear relation between the total number of matter fields and entanglement entropy is not present.

In the same way, that physics has embraced applications of machine learning, it also can be used the other way around, to justify some of its successes and further improve upon them. Following this mantra in Chapter \ref{ch:edgeofchaos} we apply the insights from statistical physics to study the computational mechanics of deep neural networks. Precisely how the initial parameter distribution for the weights and biases can lead to two different phase regimes of the network and how choosing the optimal point in this phase diagram can make the final accuracy of the network change given equal training time. We find that initializing weights and biases along the line of phase transition is necessary but not sufficient condition for optimal trainability.  


\newpage
\thispagestyle{empty}