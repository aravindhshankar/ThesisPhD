\chapter{Introduction}
\label{ch:Intro}
\par
The unifying theme in this thesis is the presence of hidden hyperbolic geometries in strongly interacting systems and their bilayers. 
\newpage     
\section{Statistical physics}
\label{sec:statPhys}
The primary goal of statistical physics is an exploration of macroscopic quantities and the calculation thereof. Often, the systems we explore are made up of many degrees of freedom, and solving them exactly is impossible. In order to do this, we will assume that the statistical average over all possible states can replace the time average. 
\par 
One of the main assumptions we make when resorting to statistical calculations instead of fully dynamically solving the system is the principle of \textit{ergodicity}. Ergodicity states that if the system is left to evolve, all accessible states will eventually be realized. This assumption helps us often turn insolvable time integrals into relatively easy and, more importantly, simulation-friendly integrals over the probability distributions of those states. For example, let us say we want to study some volume of gas in a container. At standard temperature and pressure, one liter of oxygen contains around $3\cdot 10^{22}$ oxygen molecules moving around the container. Just writing down equations of motion for all molecules would take a very long time, but no practical conclusion can be drawn even if we manage to do it. Hence we turn to the methods of statistical physics.
\par


\section{This thesis}
In the introduction, we have covered the basic ideas used later in this thesis. We started with introductory topics in thermodynamics and statistical physics, then moved to a basic introduction to Monte Carlo methods and all the required knowledge to understand our physical system's simulation design and analyze the results. The proceeding section was dedicated to the basics of machine learning, deep learning, and appropriate selection of model, loss function, and minimization method. The last section culminated in a synergy of the previously mentioned topics by combining quantum physics, Monte Carlo methods, and neural networks in neural quantum states that we used to find the ground state and its energy of lattice gauge theories.

\subsection{Chapter 1 - Thermalization in quantum systems}
The properties of closed unitary quantum systems and how they exactly thermalize have been one of the leading research questions in statistical physics for a long time. The puzzle is that a thermal ensemble is formally a mixed state, but a mixed state can never arise from unitary evolution from a pure state. The usual answer to how they thermalize is the eigenstate thermalization hypothesis (``ETH'') \cite{srednickiETF}. The hypothesis is that in generic quantum theory with many degrees of freedom, most observables will have a particular form of matrix elements after averaging, and observable will \textit{appear} to thermalize. However, recently \cite{OTHpaper} showed that ETH has to be taken with care. Even in free field theory, there are operators that \textit{appear} to relax, called operator thermalization hypothesis (OTH). Given a particular no-go condition, the retarded Green's function will typically decay exponentially unless the condition is met. Finding an operator that will satisfy this condition in a general non-integrable theory is challenging but possible. On the other hand, this job is more straightforward in integrable theories due to the extensive number of conserved quantities. We work in the transverse field Ising (TFI) model where we compare a specially designed operator $\Gamma$ that will satisfy the no-go condition with the Pauli $\sigma^z$ operator that does not satisfy it. Through the examples, we show the differences and similarities of ETH and OTH and how, despite TFI being an integrable theory, $\sigma^z$ will relax after the perturbation. Also, we have demonstrated how the no-go condition is a feature of integrability, and any minor deviation from integrability will cause $\Gamma$ to relax. Our results were later confirmed in \cite{likeOurETH}.  

\subsection{Chapter 2 - Symmetry restoration through ``registry''}
Starting from the simple Wegner gauge theory \cite{Wegner,Kogut}, Fradkin and Shenker \cite{Fradkinshenker} discovered that when an added matter field is ``in the fundamental'', meaning that there is an additional Higgs field that is also governed by $Z_2$ symmetry as the gauge fields, the Higgs phase and confining phase become one, without a phase transition. In this chapter, we propose a straightforward generalization of their lattice gauge theory that could serve as a candidate for a highly entangled state of matter. We will consider adding multiple $Z_2$ and $O(N)$ matter fields on the lattice and gauging them with a common $Z_2$ field. It will be shown how, in such a case, the Higgs phase becomes separate from the confining phase. It will be characterized by the ``registry'' order parameter, which turns out to be gauge invariant $p=2^{N_{\text{rep}-1}}$ Potts type symmetry, where $N_{\text{rep}}$ is the number of matter field copies. Interpretation for this type of symmetry is that different matter copies align their vectors locally in strictly parallel or anti-parallel ways, even in the case of continuous $O(N)$ matter fields. These theories will be studied using Monte-Carlo simulation on a $3D$ grid using the Metropolis-Hastings algorithm and annealing techniques to improve the convergence near the critical point. From the simulation results, we can discover some unidentified ``pseudo-universality'' associated with the form of the phase diagram for various numbers of matter field theories.

\subsection{Chapter 3 - Entanglement entropy of lattice gauge theories}
Building further on the work from the previous chapter, we will study entanglement entropy in the neural network representation of the above lattice gauge theories, now considered as quantum theories in one lower dimension. Following the seminal work of Carleo and Troeyer \cite{carleoTroyer}, we will construct neural quantum states as the representation of our theory using a variational wavefunction based on Restricted Boltzmann Machines used in Machine Learning. Using ideas from tensor networks that the bond dimension represents the upper bound on the amount of entanglement a state can have, we will postulate that by increasing the number of matter fields, ground state entanglement entropy of our lattice gauge theory will increase as the ratio of hidden vs. visible nodes. We have tested our hypothesis in the case of $2, 3$, and $4$ matter fields. Within the achievable computational limits, the results are puzzling. Even though increasing the number of variational parameters improved the energy of the ground state, the impact on the entanglement entropy is less than obvious. Curves of entanglement entropy for different system sizes look the same up to the statistical errors. 

\subsection{Chapter 4 - Phase space and efficient learning of deep neural networks}
This chapter combines some statistical physics insights into machine learning with the computational mechanics of deep random feedforward neural networks. In recent times with the ever-growing amount of available data, neural networks have become one of the de-facto methods for analyzing and processing vast amounts of data \cite{Mehta_2019}. One of the reasons why these methods became so popular is their ability to express any function with a relatively small number of parameters \cite{MLintro} and the ease with which this \textit{expressivity} can be increased by adding more depth. This easy fix does not come for free; deep neural networks generally require more training computations. Specifically, they suffer from exploding or vanishing derivatives in optimizing the parameters. The phase space of deep random feedforward neural networks is characterized by the variance of initial weights and the variance of initial bias. Following previous work \cite{arxiv.1606.05340,2016arXiv161101232S} that demonstrated the existence of order-to-chaos regime change in this phase space, we will examine the behavior of the pre-and post-activations in terms of their distributions and also final accuracy on classification task such are MNIST and CIFAR10. The phase boundary dividing these two regimes is called the edge of chaos (EOC). We demonstrate that for the $\tanh$ activation function, not all points along the EOC yield the same learning efficiency. In the case of shallow and narrow neural networks, we define the line of uniformity (LOU), a set of points for which the final layer post-activation values are distributed uniformly, i.e., with maximal entropy. We show that moving away to the right from LOU and drastically increasing initial variances means that gradient saturation will start impeding optimization over parameters, i.e., the learning process.    
